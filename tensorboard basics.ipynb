{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7be08691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import numpy as np\n",
    "import pandas  as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "torch.set_printoptions(linewidth=120)\n",
    "torch.set_grad_enabled(True)\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54d26128",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network,self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels= 1, out_channels= 6 , kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels= 6, out_channels= 12 , kernel_size=5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features= 12*4*4 , out_features= 120)\n",
    "        self.fc2 = nn.Linear(in_features= 120, out_features= 60)\n",
    "        self.out = nn.Linear(in_features= 60, out_features= 10)\n",
    "\n",
    "    def forward(self,t):\n",
    "        \n",
    "        #1 inpput layer \n",
    "        \n",
    "        t = t\n",
    "        \n",
    "        #2 hidden conv layer\n",
    "        \n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size = 2 , stride = 2)\n",
    "        \n",
    "        #3 hidden conv layer \n",
    "        \n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size = 2 , stride = 2)\n",
    "        \n",
    "        #4 Linear layer \n",
    "        \n",
    "        t = t.reshape(-1, 12*4*4)     #flattening is hapening here\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        \n",
    "        #5 Linear layer \n",
    "        \n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "        \n",
    "        #6 output layer\n",
    "        \n",
    "        t = self.out(t)\n",
    "        # t = F.softmax(t,dim = 0)    but this line is not required because we will predict the output later and softmax will be used explicitly later\n",
    "        return t\n",
    "        \n",
    "#loading data from url of Mnist\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "        root = './data'\n",
    "        ,train = True\n",
    "        ,download=True\n",
    "        ,transform = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "    \n",
    ")\n",
    "\n",
    "        \n",
    "data_loader = torch.utils.data.DataLoader(train_set,\n",
    "                    batch_size= 100                  \n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0631d7ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1000, 5000, 10000], [0.001, 0.0001], [True, False]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import product\n",
    "parameters = dict(\n",
    "             batch_size = [1000,5000,10000],\n",
    "lr = [.001,.0001],\n",
    "shuffle = [True,False]\n",
    ")\n",
    "param_values = [v for v in parameters.values()]\n",
    "param_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71896f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds,labels):\n",
    "    \n",
    "    return preds.argmax(dim = 1).eq(labels).sum().item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a75ddca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 0.001 True\n",
      "1000 0.001 False\n",
      "1000 0.0001 True\n",
      "1000 0.0001 False\n",
      "5000 0.001 True\n",
      "5000 0.001 False\n",
      "5000 0.0001 True\n",
      "5000 0.0001 False\n",
      "10000 0.001 True\n",
      "10000 0.001 False\n",
      "10000 0.0001 True\n",
      "10000 0.0001 False\n"
     ]
    }
   ],
   "source": [
    "for batch_size,lr,shuffle in product(*param_values):\n",
    "    print(batch_size,lr,shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef57eea",
   "metadata": {},
   "source": [
    "# Another Aproach is to use Runbuilder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c53e77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from collections import namedtuple\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cb77dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = OrderedDict(\n",
    "            \n",
    "            lr = [.01,.001],\n",
    "            batch_size = [100,1000,10000],\n",
    "            shuffle = [True,False]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b946fafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the class\n",
    "\n",
    "class RunBuilder():\n",
    "    @staticmethod\n",
    "    \n",
    "    def get_runs(params):\n",
    "        \n",
    "        \n",
    "        Run = namedtuple('Run',params.keys())\n",
    "        \n",
    "        runs = []\n",
    "        \n",
    "        for v in product(*params.values()):\n",
    "            runs.append(Run(*v))\n",
    "        return runs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4ecb1d79",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Run(lr=0.01, batch_size=100, shuffle=True),\n",
       " Run(lr=0.01, batch_size=100, shuffle=False),\n",
       " Run(lr=0.01, batch_size=1000, shuffle=True),\n",
       " Run(lr=0.01, batch_size=1000, shuffle=False),\n",
       " Run(lr=0.01, batch_size=10000, shuffle=True),\n",
       " Run(lr=0.01, batch_size=10000, shuffle=False),\n",
       " Run(lr=0.001, batch_size=100, shuffle=True),\n",
       " Run(lr=0.001, batch_size=100, shuffle=False),\n",
       " Run(lr=0.001, batch_size=1000, shuffle=True),\n",
       " Run(lr=0.001, batch_size=1000, shuffle=False),\n",
       " Run(lr=0.001, batch_size=10000, shuffle=True),\n",
       " Run(lr=0.001, batch_size=10000, shuffle=False)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params1 = OrderedDict(\n",
    "            \n",
    "            batch_size = [100,1000],\n",
    "            lr = [.01,.001],\n",
    "            device = ['cuda', 'cpu']\n",
    ") \n",
    "Runs = RunBuilder.get_runs(params1)\n",
    "Runs = RunBuilder.get_runs(params)\n",
    "Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92b9737c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 0.001 True\n",
      "epoch: 0 total loss: 97761.58267259598 total correct: 25695\n",
      "epoch: 1 total loss: 47783.6109995842 total correct: 42002\n",
      "epoch: 2 total loss: 41220.34651041031 total correct: 44318\n",
      "epoch: 3 total loss: 37806.72359466553 total correct: 45578\n",
      "epoch: 4 total loss: 35221.113204956055 total correct: 46653\n",
      "epoch: 5 total loss: 33312.54795193672 total correct: 47339\n",
      "1000 0.001 False\n",
      "epoch: 0 total loss: 33592.165023088455 total correct: 47358\n",
      "epoch: 1 total loss: 31433.259904384613 total correct: 48341\n",
      "epoch: 2 total loss: 30614.614009857178 total correct: 48764\n",
      "epoch: 3 total loss: 29818.369656801224 total correct: 49154\n",
      "epoch: 4 total loss: 29040.54895043373 total correct: 49481\n",
      "epoch: 5 total loss: 28308.53235721588 total correct: 49784\n",
      "1000 0.0001 True\n",
      "epoch: 0 total loss: 27340.08577466011 total correct: 50247\n",
      "epoch: 1 total loss: 27148.66927266121 total correct: 50338\n",
      "epoch: 2 total loss: 27016.471296548843 total correct: 50361\n",
      "epoch: 3 total loss: 26947.718918323517 total correct: 50437\n",
      "epoch: 4 total loss: 26797.480016946793 total correct: 50444\n",
      "epoch: 5 total loss: 26703.20811867714 total correct: 50512\n",
      "1000 0.0001 False\n",
      "epoch: 0 total loss: 26594.662368297577 total correct: 50540\n",
      "epoch: 1 total loss: 26486.377865076065 total correct: 50554\n",
      "epoch: 2 total loss: 26386.821568012238 total correct: 50601\n",
      "epoch: 3 total loss: 26285.753399133682 total correct: 50643\n",
      "epoch: 4 total loss: 26187.11856007576 total correct: 50684\n",
      "epoch: 5 total loss: 26089.604079723358 total correct: 50735\n",
      "5000 0.001 True\n",
      "epoch: 0 total loss: 33233.74316096306 total correct: 47972\n",
      "epoch: 1 total loss: 27857.301384210587 total correct: 49865\n",
      "epoch: 2 total loss: 26666.769981384277 total correct: 50522\n",
      "epoch: 3 total loss: 26119.96054649353 total correct: 50796\n",
      "epoch: 4 total loss: 25886.82532310486 total correct: 50836\n",
      "epoch: 5 total loss: 25744.790583848953 total correct: 50900\n",
      "5000 0.001 False\n",
      "epoch: 0 total loss: 29875.578135252 total correct: 49194\n",
      "epoch: 1 total loss: 26876.289546489716 total correct: 50415\n",
      "epoch: 2 total loss: 26058.072447776794 total correct: 50789\n",
      "epoch: 3 total loss: 25579.64250445366 total correct: 50926\n",
      "epoch: 4 total loss: 25316.10056757927 total correct: 50992\n",
      "epoch: 5 total loss: 25169.68995332718 total correct: 51046\n",
      "5000 0.0001 True\n",
      "epoch: 0 total loss: 25038.317441940308 total correct: 51085\n",
      "epoch: 1 total loss: 24958.0118060112 total correct: 51155\n",
      "epoch: 2 total loss: 24914.653599262238 total correct: 51151\n",
      "epoch: 3 total loss: 24870.41637301445 total correct: 51190\n",
      "epoch: 4 total loss: 24832.37847685814 total correct: 51179\n",
      "epoch: 5 total loss: 24802.673310041428 total correct: 51167\n",
      "5000 0.0001 False\n",
      "epoch: 0 total loss: 24819.297045469284 total correct: 51163\n",
      "epoch: 1 total loss: 24775.101393461227 total correct: 51158\n",
      "epoch: 2 total loss: 24713.170379400253 total correct: 51170\n",
      "epoch: 3 total loss: 24683.33512544632 total correct: 51210\n",
      "epoch: 4 total loss: 24652.238488197327 total correct: 51201\n",
      "epoch: 5 total loss: 24613.434970378876 total correct: 51216\n",
      "10000 0.001 True\n",
      "epoch: 0 total loss: 28976.143300533295 total correct: 49521\n",
      "epoch: 1 total loss: 26379.009187221527 total correct: 50584\n",
      "epoch: 2 total loss: 25716.71724319458 total correct: 50754\n",
      "epoch: 3 total loss: 25209.935009479523 total correct: 51065\n",
      "epoch: 4 total loss: 24890.645742416382 total correct: 51129\n",
      "epoch: 5 total loss: 24654.06686067581 total correct: 51190\n",
      "10000 0.001 False\n",
      "epoch: 0 total loss: 30474.347174167633 total correct: 48758\n",
      "epoch: 1 total loss: 27040.163576602936 total correct: 50188\n",
      "epoch: 2 total loss: 25986.68932914734 total correct: 50545\n",
      "epoch: 3 total loss: 25153.83541584015 total correct: 51095\n",
      "epoch: 4 total loss: 24825.536012649536 total correct: 51163\n",
      "epoch: 5 total loss: 24521.205723285675 total correct: 51297\n",
      "10000 0.0001 True\n",
      "epoch: 0 total loss: 24294.72118616104 total correct: 51387\n",
      "epoch: 1 total loss: 24208.424985408783 total correct: 51383\n",
      "epoch: 2 total loss: 24158.74481201172 total correct: 51426\n",
      "epoch: 3 total loss: 24103.645086288452 total correct: 51439\n",
      "epoch: 4 total loss: 24075.31589269638 total correct: 51395\n",
      "epoch: 5 total loss: 24026.966392993927 total correct: 51438\n",
      "10000 0.0001 False\n",
      "epoch: 0 total loss: 24053.832292556763 total correct: 51443\n",
      "epoch: 1 total loss: 24002.691507339478 total correct: 51441\n",
      "epoch: 2 total loss: 23953.847587108612 total correct: 51465\n",
      "epoch: 3 total loss: 23914.64114189148 total correct: 51460\n",
      "epoch: 4 total loss: 23888.38291168213 total correct: 51470\n",
      "epoch: 5 total loss: 23864.926397800446 total correct: 51468\n"
     ]
    }
   ],
   "source": [
    "test_net = Network()\n",
    "epochs = 6\n",
    "for batch_size,lr,shuffle in product(*param_values):\n",
    "    print(batch_size,lr,shuffle)\n",
    "    comment = f'batch size = {batch_size},lr = {lr},shuffle = {shuffle}'\n",
    "    \n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size ,shuffle=shuffle)\n",
    "    images,labels = next(iter(train_loader))\n",
    "    tb = SummaryWriter(comment = comment)\n",
    "    grid = torchvision.utils.make_grid(images)\n",
    "    \n",
    "\n",
    "    tb = SummaryWriter(comment = comment)\n",
    "    tb.add_image('images',grid)\n",
    "    tb.add_graph(test_net,images)\n",
    "\n",
    "    optimizer = optim.Adam(test_net.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        total_loss = 0\n",
    "        total_correct = 0\n",
    "        for batch in train_loader:                      # Get Batch\n",
    "            images, labels = batch\n",
    "\n",
    "            preds = test_net(images) # Pass Batch\n",
    "            loss = F.cross_entropy(preds, labels) # Calculate Loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward() # Calculate Gradients\n",
    "            optimizer.step() # Update Weights\n",
    "\n",
    "            total_loss += loss.item()*images.shape[0]               #using it to generalise the loss for different batch sizes\n",
    "            total_correct += get_num_correct(preds,labels)\n",
    "\n",
    "        tb.add_scalar(\"loss\",total_loss,epoch)\n",
    "        tb.add_scalar(\"total correct\", total_correct,epoch)\n",
    "        tb.add_scalar(\"Accuracy\", total_correct/len(train_set), epoch)\n",
    "\n",
    "        tb.add_histogram(\"conv1 bias\", test_net.conv1.bias, epoch)\n",
    "        tb.add_histogram(\"conv1.weight\", test_net.conv1.weight,epoch)\n",
    "        tb.add_histogram(\"conv1.weight.grad\", test_net.conv1.weight.grad,epoch)\n",
    "\n",
    "        print(\"epoch:\", epoch  , \"total loss:\", total_loss, \"total correct:\" ,total_correct )\n",
    "\n",
    "tb.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882dcd2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d37eed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
