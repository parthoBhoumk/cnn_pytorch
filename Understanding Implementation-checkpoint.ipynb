{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e3405c0",
   "metadata": {},
   "source": [
    "# Cnn flatten operation visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "924781b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1616b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([\n",
    "    [1,1,1,1],\n",
    "    [1,1,1,1],\n",
    "    [1,1,1,1],\n",
    "    [1,1,1,1]\n",
    "])\n",
    "\n",
    "t2 = torch.tensor([\n",
    "    [2,2,2,2],\n",
    "    [2,2,2,2],\n",
    "    [2,2,2,2],\n",
    "    [2,2,2,2]\n",
    "])\n",
    "\n",
    "t3 = torch.tensor([\n",
    "    [3,3,3,3],\n",
    "    [3,3,3,3],\n",
    "    [3,3,3,3],\n",
    "    [3,3,3,3]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cf316b",
   "metadata": {},
   "source": [
    "t1,t2,t3 are three 4*4 images and in a batch \n",
    "they are fed to the network....so we have to combine\n",
    "the imgaes and create 3 axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf6bf12f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 4])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.stack((t1,t2,t3))\n",
    "len(t.shape) #rank\n",
    "\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7141ffbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# but there is no colored channel in this tensor so \n",
    "# we have to reshape the tensors in such a way\n",
    "\n",
    "t = t.reshape(3,1,4,4)    #(batch size,color channel.height.width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3595a974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1, 1],\n",
       "         [1, 1, 1, 1],\n",
       "         [1, 1, 1, 1],\n",
       "         [1, 1, 1, 1]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets access each component \n",
    "t[0]   #--> we get the first image.its greyscale so only one color channel is here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0da3bcc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0][0] # first colored channel of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ca4c8d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0][0][0]  #first row of pixels of the first color channel of the first image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a029c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t[0][0][0][0]   # first pixel of first row of first color channel of first image in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef1044aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "799a06f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.flatten()  #built in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c70da539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 16])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # but we cant flatten all images together rather we can flatten one color channel image and use it in our cnn model for better output\n",
    "    \n",
    "    \n",
    "t.flatten(start_dim = 1).shape   # star_dim tells which axis it should start to flaten first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d23e95c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "        [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.flatten(start_dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "48b53259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "        [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.reshape(3,16)    #flattening using the reshape method only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d474bf",
   "metadata": {},
   "source": [
    "# Element-Wise Tensor Operations For Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "971ed325",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.tensor([\n",
    "    [1,2],\n",
    "    [3,4]\n",
    "],dtype = torch.float32)\n",
    "\n",
    "\n",
    "t2 = torch.tensor([\n",
    "    [9,8],\n",
    "    [7,6]\n",
    "], dtype = torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c5543310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10., 10.],\n",
       "        [10., 10.]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1+t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1c05f996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 1.0000],\n",
       "        [1.5000, 2.0000]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for element wise operation to be done tensors have to be of the same shape but here it is not the same\n",
    "\n",
    "t1 + 2\n",
    "t1*2\n",
    "t1-2\n",
    "t1.add(2)\n",
    "t1.div(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "384a7d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 4.],\n",
       "        [5., 6.]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the scaler value is being broadcasted and then operated with tensor\n",
    "#valu t1.\n",
    "np.broadcast_to(2,t1.shape)\n",
    "t1 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a32146f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#another example of broadcasting \n",
    "\n",
    "t2 = torch.tensor([2,4])\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0114155b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8901a4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 6.],\n",
       "        [5., 8.]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1+t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "767f6bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here t2 is not the same shape as t1 but broadcasting made it possible to add them both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3ba0a036",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([\n",
    "    [0,8,3],\n",
    "    [0,5,0],\n",
    "    [4,-2,6]\n",
    "], dtype= torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "312ca28d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bool"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.eq(0).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dfd00d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True,  True,  True],\n",
       "        [ True,  True,  True],\n",
       "        [ True, False,  True]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.ge(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "deb86934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False],\n",
       "        [ True, False,  True],\n",
       "        [False,  True, False]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.le(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "36924ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False],\n",
       "        [False, False, False],\n",
       "        [False,  True, False]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.lt(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "34da5182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 8., 3.],\n",
       "        [0., 5., 0.],\n",
       "        [4., 2., 6.]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3e6499e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 2.8284, 1.7321],\n",
       "        [0.0000, 2.2361, 0.0000],\n",
       "        [2.0000,    nan, 2.4495]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "583b3229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0., -8., -3.],\n",
       "        [-0., -5., -0.],\n",
       "        [-4.,  2., -6.]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.neg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "eb128616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0., 64.,  9.],\n",
       "        [ 0., 25.,  0.],\n",
       "        [16.,  4., 36.]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.pow(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8583cef4",
   "metadata": {},
   "source": [
    "# Tensor Reduction Ops For Deep Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "27693126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6., 6., 6., 6.])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([\n",
    "    [1,1,1,1],\n",
    "    [2,2,2,2],\n",
    "    [3,3,3,3]\n",
    "],dtype = torch.float32)\n",
    "t.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5515fbb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.,  8., 12.])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sum(1)      #reducing along a particular axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e3832aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2.])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.mean(0)    #reducing using mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3aca49a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.mean(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "82901dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#argmax \n",
    "\n",
    "t = torch.tensor([\n",
    "    [1,0,0,2],\n",
    "    [2,4,1,2],\n",
    "    [0,1,4,12]\n",
    "],dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b34578d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cbd77e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1b54db36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.,  0.,  0.,  2.,  2.,  4.,  1.,  2.,  0.,  1.,  4., 12.])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4cbc7a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([ 2.,  4.,  4., 12.]),\n",
       "indices=tensor([1, 1, 2, 2]))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max(dim =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3cf7d18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([ 2.,  4., 12.]),\n",
       "indices=tensor([3, 1, 3]))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max(dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1295c5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.tensor([\n",
    "    [1,2,3],\n",
    "    [4,5,6],\n",
    "    [7,8,9]\n",
    "],dtype = torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "50ace31a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5.)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b7d968f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8e8cd2c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 5., 6.], dtype=float32)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.mean(0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6aaf4480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4.0, 5.0, 6.0]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.mean(0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8042af14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
